From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/nvme/target/rdma.c

Change-Id: Ia376abb580ccb072767777cba2d20efc078fa774
---
 drivers/nvme/target/rdma.c | 80 ++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 80 insertions(+)

--- a/drivers/nvme/target/rdma.c
+++ b/drivers/nvme/target/rdma.c
@@ -140,8 +140,13 @@ static bool nvmet_rdma_use_srq;
 module_param_named(use_srq, nvmet_rdma_use_srq, bool, 0444);
 MODULE_PARM_DESC(use_srq, "Use shared receive queue.");
 
+#ifdef HAVE_PARAM_OPS_ULLONG
 static unsigned long long nvmet_rdma_offload_mem_start = 0;
 module_param_named(offload_mem_start, nvmet_rdma_offload_mem_start, ullong, 0444);
+#else
+static unsigned long nvmet_rdma_offload_mem_start = 0;
+module_param_named(offload_mem_start, nvmet_rdma_offload_mem_start, ulong, 0444);
+#endif
 MODULE_PARM_DESC(offload_mem_start,
 		 "Start address of the memory dedicated for P2P data transfer. If not set, the driver will allocate 1MB staging buffer per offload context."
 		 "Using bigger staging buffer will improve performance. Must be contiguous and aligned to" __stringify(PAGE_SIZE) "(default:0)");
@@ -246,6 +251,63 @@ nvmet_rdma_put_rsp(struct nvmet_rdma_rsp
 	spin_unlock_irqrestore(&rsp->queue->rsps_lock, flags);
 }
 
+#ifndef HAVE_SGL_FREE
+static void nvmet_rdma_free_sgl(struct scatterlist *sgl, unsigned int nents)
+{
+	struct scatterlist *sg;
+	int count;
+
+	if (!sgl || !nents)
+		return;
+
+	for_each_sg(sgl, sg, nents, count)
+		__free_page(sg_page(sg));
+	kfree(sgl);
+}
+
+#endif
+#ifndef HAVE_SGL_ALLOC
+static int nvmet_rdma_alloc_sgl(struct scatterlist **sgl, unsigned int *nents,
+               u32 length)
+{
+	struct scatterlist *sg;
+	struct page *page;
+	unsigned int nent;
+	int i = 0;
+
+	nent = DIV_ROUND_UP(length, PAGE_SIZE);
+	sg = kmalloc_array(nent, sizeof(struct scatterlist), GFP_KERNEL);
+	if (!sg)
+		goto out;
+
+	sg_init_table(sg, nent);
+
+	while (length) {
+		u32 page_len = min_t(u32, length, PAGE_SIZE);
+
+	        page = alloc_page(GFP_KERNEL);
+		if (!page)
+	                goto out_free_pages;
+
+	        sg_set_page(&sg[i], page, page_len, 0);
+		length -= page_len;
+		i++;
+	}
+	*sgl = sg;
+	*nents = nent;
+	return 0;
+
+out_free_pages:
+	while (i > 0) {
+		i--;
+		__free_page(sg_page(&sg[i]));
+	}
+	kfree(sg);
+out:
+	return NVME_SC_INTERNAL;
+}
+
+#endif
 static int nvmet_rdma_alloc_cmd(struct nvmet_rdma_device *ndev,
 			struct nvmet_rdma_cmd *c, bool admin)
 {
@@ -492,7 +554,11 @@ static void nvmet_rdma_release_rsp(struc
 	}
 
 	if (rsp->req.sg != &rsp->cmd->inline_sg)
+#ifdef HAVE_SGL_FREE
 		sgl_free(rsp->req.sg);
+#else
+		nvmet_rdma_free_sgl(rsp->req.sg, rsp->req.sg_cnt);
+#endif
 
 	if (unlikely(!list_empty_careful(&queue->rsp_wr_wait_list)))
 		nvmet_rdma_process_wr_wait_list(queue);
@@ -633,14 +699,24 @@ static u16 nvmet_rdma_map_sgl_keyed(stru
 	u32 len = get_unaligned_le24(sgl->length);
 	u32 key = get_unaligned_le32(sgl->key);
 	int ret;
+#ifndef HAVE_SGL_ALLOC
+	u16 status;
+#endif
 
 	/* no data command? */
 	if (!len)
 		return 0;
 
+#ifdef HAVE_SGL_ALLOC
 	rsp->req.sg = sgl_alloc(len, GFP_KERNEL, &rsp->req.sg_cnt);
 	if (!rsp->req.sg)
 		return NVME_SC_INTERNAL;
+#else
+	status = nvmet_rdma_alloc_sgl(&rsp->req.sg, &rsp->req.sg_cnt,
+			len);
+	if (status)
+		return status;
+#endif
 
 	ret = rdma_rw_ctx_init(&rsp->rw, cm_id->qp, cm_id->port_num,
 			rsp->req.sg, rsp->req.sg_cnt, 0, addr, key,
@@ -1588,6 +1664,7 @@ static void nvmet_rdma_remove_port(struc
 static void nvmet_rdma_disc_port_addr(struct nvmet_req *req,
 		struct nvmet_port *nport, char *traddr)
 {
+#ifdef HAVE_INET_ADDR_IS_ANY
 	struct nvmet_rdma_port *port = nport->priv;
 	struct rdma_cm_id *cm_id = port->cm_id;
 
@@ -1601,6 +1678,9 @@ static void nvmet_rdma_disc_port_addr(st
 	} else {
 		memcpy(traddr, nport->disc_addr.traddr, NVMF_TRADDR_SIZE);
 	}
+#else
+	memcpy(traddr, nport->disc_addr.traddr, NVMF_TRADDR_SIZE);
+#endif
 }
 
 static void nvmet_rdma_add_one(struct ib_device *ib_device)
