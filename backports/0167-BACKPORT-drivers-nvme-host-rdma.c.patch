From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/nvme/host/rdma.c

Change-Id: Ida096c45b7e95be72d44456c70c5a71ee918def4
---
 drivers/nvme/host/rdma.c | 166 +++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 166 insertions(+)

--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -24,13 +24,19 @@
 #include <linux/string.h>
 #include <linux/atomic.h>
 #include <linux/blk-mq.h>
+#ifdef HAVE_BLK_MQ_MAP_QUEUES
 #include <linux/blk-mq-rdma.h>
+#endif
 #include <linux/types.h>
 #include <linux/list.h>
 #include <linux/mutex.h>
 #include <linux/scatterlist.h>
 #include <linux/nvme.h>
 #include <asm/unaligned.h>
+#ifdef HAVE_SCSI_MAX_SG_SEGMENTS
+#include <scsi/scsi.h>
+#endif
+#include <linux/refcount.h>
 
 #include <rdma/ib_verbs.h>
 #include <rdma/rdma_cm.h>
@@ -148,8 +154,13 @@ static int nvme_rdma_cm_handler(struct r
 		struct rdma_cm_event *event);
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
 
+#ifdef HAVE_BLK_MQ_TAG_SET_HAS_CONST_POS
 static const struct blk_mq_ops nvme_rdma_mq_ops;
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
+#else
+static struct blk_mq_ops nvme_rdma_mq_ops;
+static struct blk_mq_ops nvme_rdma_admin_mq_ops;
+#endif
 
 /* XXX: really should move to a generic header sooner or later.. */
 static inline void put_unaligned_le24(u32 val, u8 *p)
@@ -268,26 +279,57 @@ static int nvme_rdma_create_qp(struct nv
 	return ret;
 }
 
+#ifdef HAVE_BLK_MQ_OPS_EXIT_REQUEST_HAS_3_PARAMS
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
+#else
+static void __nvme_rdma_exit_request(struct nvme_rdma_ctrl *ctrl,
+				     struct request *rq, unsigned int queue_idx)
+#endif
 {
+#ifdef HAVE_BLK_MQ_OPS_EXIT_REQUEST_HAS_3_PARAMS
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
+#endif
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+#ifdef HAVE_BLK_MQ_OPS_EXIT_REQUEST_HAS_3_PARAMS
 	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+#endif
 	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
 	struct nvme_rdma_device *dev = queue->device;
 
 	nvme_rdma_free_qe(dev->dev, &req->sqe, sizeof(struct nvme_command),
 			DMA_TO_DEVICE);
 }
+#ifndef HAVE_BLK_MQ_OPS_EXIT_REQUEST_HAS_3_PARAMS
+static void nvme_rdma_exit_request(void *data, struct request *rq,
+				   unsigned int hctx_idx, unsigned int rq_idx)
+{
+	__nvme_rdma_exit_request(data, rq, hctx_idx + 1);
+}
+
+static void nvme_rdma_exit_admin_request(void *data, struct request *rq,
+					 unsigned int hctx_idx, unsigned int rq_idx)
+{
+	__nvme_rdma_exit_request(data, rq, 0);
+}
+#endif
 
+#ifdef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
+#else
+static int __nvme_rdma_init_request(struct nvme_rdma_ctrl *ctrl,
+				    struct request *rq, unsigned int queue_idx)
+#endif
 {
+#ifdef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
+#endif
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+#ifdef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
 	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+#endif
 	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
 	struct nvme_rdma_device *dev = queue->device;
 	struct ib_device *ibdev = dev->dev;
@@ -302,6 +344,21 @@ static int nvme_rdma_init_request(struct
 
 	return 0;
 }
+#ifndef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
+static int nvme_rdma_init_request(void *data, struct request *rq,
+				  unsigned int hctx_idx, unsigned int rq_idx,
+				  unsigned int numa_node)
+{
+	return __nvme_rdma_init_request(data, rq, hctx_idx + 1);
+}
+
+static int nvme_rdma_init_admin_request(void *data, struct request *rq,
+					unsigned int hctx_idx, unsigned int rq_idx,
+					unsigned int numa_node)
+{
+	return __nvme_rdma_init_request(data, rq, 0);
+}
+#endif
 
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
@@ -435,6 +492,9 @@ static int nvme_rdma_create_queue_ib(str
 	const int cq_factor = send_wr_factor + 1;	/* + RECV */
 	int comp_vector, idx = nvme_rdma_queue_idx(queue);
 	int ret;
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+	enum ib_mr_type		mr_type;
+#endif
 
 	queue->device = nvme_rdma_find_get_device(queue->cm_id);
 	if (!queue->device) {
@@ -444,11 +504,15 @@ static int nvme_rdma_create_queue_ib(str
 	}
 	ibdev = queue->device->dev;
 
+#ifdef HAVE_BLK_MQ_ALLOC_REQUEST_HCTX
 	/*
 	 * Spread I/O queues completion vectors according their queue index.
 	 * Admin queues can always go on completion vector 0.
 	 */
 	comp_vector = idx == 0 ? idx : idx - 1;
+#else
+	comp_vector = queue->ctrl->ctrl.instance % ibdev->num_comp_vectors;
+#endif
 
 	/* +1 for ib_stop_cq */
 	queue->ib_cq = ib_alloc_cq(ibdev, queue,
@@ -470,9 +534,20 @@ static int nvme_rdma_create_queue_ib(str
 		goto out_destroy_qp;
 	}
 
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+	if (ibdev->attrs.device_cap_flags & IB_DEVICE_SG_GAPS_REG)
+		mr_type = IB_MR_TYPE_SG_GAPS;
+	else
+		mr_type = IB_MR_TYPE_MEM_REG;
+#endif
+
 	ret = ib_mr_pool_init(queue->qp, &queue->qp->rdma_mrs,
 			      queue->queue_size,
+#ifdef HAVE_BLK_QUEUE_VIRT_BOUNDARY
 			      IB_MR_TYPE_MEM_REG,
+#else
+			      mr_type,
+#endif
 			      nvme_rdma_get_max_fr_pages(ibdev));
 	if (ret) {
 		dev_err(queue->ctrl->ctrl.device,
@@ -694,12 +769,19 @@ static struct blk_mq_tag_set *nvme_rdma_
 		set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
 		set->reserved_tags = 2; /* connect + keep-alive */
 		set->numa_node = NUMA_NO_NODE;
+#ifdef HAVE_SCSI_MAX_SG_SEGMENTS
+		set->cmd_size = sizeof(struct nvme_rdma_request) +
+			SCSI_MAX_SG_SEGMENTS * sizeof(struct scatterlist);
+#else
 		set->cmd_size = sizeof(struct nvme_rdma_request) +
 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+#endif
 		set->driver_data = ctrl;
 		set->nr_hw_queues = 1;
 		set->timeout = ADMIN_TIMEOUT;
+#ifdef HAVE_BLK_MQ_F_NO_SCHED
 		set->flags = BLK_MQ_F_NO_SCHED;
+#endif
 	} else {
 		set = &ctrl->tag_set;
 		memset(set, 0, sizeof(*set));
@@ -708,8 +790,13 @@ static struct blk_mq_tag_set *nvme_rdma_
 		set->reserved_tags = 1; /* fabric connect */
 		set->numa_node = NUMA_NO_NODE;
 		set->flags = BLK_MQ_F_SHOULD_MERGE;
+#ifdef HAVE_SCSI_MAX_SG_SEGMENTS
+		set->cmd_size = sizeof(struct nvme_rdma_request) +
+			SCSI_MAX_SG_SEGMENTS * sizeof(struct scatterlist);
+#else
 		set->cmd_size = sizeof(struct nvme_rdma_request) +
 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+#endif
 		set->driver_data = ctrl;
 		set->nr_hw_queues = nctrl->queue_count - 1;
 		set->timeout = NVME_IO_TIMEOUT;
@@ -796,6 +883,10 @@ static int nvme_rdma_configure_admin_que
 	ctrl->ctrl.sqsize =
 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
 
+#ifndef HAVE_BLK_QUEUE_VIRT_BOUNDARY
+	if (ctrl->device->dev->attrs.device_cap_flags & IB_DEVICE_SG_GAPS_REG)
+		ctrl->ctrl.sg_gaps_support = true;
+#endif
 	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
 	if (error)
 		goto out_stop_queue;
@@ -856,8 +947,10 @@ static int nvme_rdma_configure_io_queues
 			goto out_free_tag_set;
 		}
 	} else {
+#ifdef HAVE_BLK_MQ_UPDATE_NR_HW_QUEUES
 		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
 			ctrl->ctrl.queue_count - 1);
+#endif
 	}
 
 	ret = nvme_rdma_start_io_queues(ctrl);
@@ -1022,7 +1115,11 @@ static void nvme_rdma_error_recovery_wor
 		nvme_rdma_destroy_io_queues(ctrl, false);
 	}
 
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+#else
+	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
+#endif
 	nvme_rdma_stop_queue(&ctrl->queues[0]);
 	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
 				nvme_cancel_request, &ctrl->ctrl);
@@ -1032,7 +1129,11 @@ static void nvme_rdma_error_recovery_wor
 	 * queues are not a live anymore, so restart the queues to fail fast
 	 * new IO
 	 */
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+#else
+	blk_mq_start_stopped_hw_queues(ctrl->ctrl.admin_q, true);
+#endif
 	nvme_start_queues(&ctrl->ctrl);
 
 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
@@ -1113,7 +1214,11 @@ static void nvme_rdma_unmap_data(struct
 	struct nvme_rdma_device *dev = queue->device;
 	struct ib_device *ibdev = dev->dev;
 
+#ifdef HAVE_BLK_RQ_NR_PAYLOAD_BYTES
 	if (!blk_rq_payload_bytes(rq))
+#else
+	if (!nvme_map_len(rq))
+#endif
 		return;
 
 	if (req->mr) {
@@ -1228,12 +1333,23 @@ static int nvme_rdma_map_data(struct nvm
 
 	c->common.flags |= NVME_CMD_SGL_METABUF;
 
+#ifdef HAVE_BLK_RQ_NR_PAYLOAD_BYTES
 	if (!blk_rq_payload_bytes(rq))
+#else
+	if (!nvme_map_len(rq))
+#endif
 		return nvme_rdma_set_sg_null(c);
 
 	req->sg_table.sgl = req->first_sgl;
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_4_PARAMS
+	ret = sg_alloc_table_chained(&req->sg_table,
+			blk_rq_nr_phys_segments(rq),
+			GFP_ATOMIC,
+			req->sg_table.sgl);
+#else
 	ret = sg_alloc_table_chained(&req->sg_table,
 			blk_rq_nr_phys_segments(rq), req->sg_table.sgl);
+#endif
 	if (ret)
 		return -ENOMEM;
 
@@ -1248,7 +1364,11 @@ static int nvme_rdma_map_data(struct nvm
 
 	if (count == 1) {
 		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
+#ifdef HAVE_BLK_RQ_NR_PAYLOAD_BYTES
 		    blk_rq_payload_bytes(rq) <=
+#else
+		    nvme_map_len(rq) <=
+#endif
 				nvme_rdma_inline_data_size(queue)) {
 			ret = nvme_rdma_map_sg_inline(queue, req, c);
 			goto out;
@@ -1667,7 +1787,11 @@ nvme_rdma_timeout(struct request *rq, bo
 	/* queue error recovery */
 	nvme_rdma_error_recovery(req->queue->ctrl);
 
+#ifdef	HAVE_BLK_EH_DONE
 	return BLK_EH_DONE;
+#else
+	return BLK_EH_RESET_TIMER;
+#endif
 }
 
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
@@ -1727,6 +1851,7 @@ err:
 	return BLK_STS_IOERR;
 }
 
+#ifdef HAVE_BLK_MQ_POLL
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1747,6 +1872,7 @@ static int nvme_rdma_poll(struct blk_mq_
 
 	return found;
 }
+#endif
 
 static void nvme_rdma_complete_rq(struct request *rq)
 {
@@ -1756,29 +1882,61 @@ static void nvme_rdma_complete_rq(struct
 	nvme_complete_rq(rq);
 }
 
+#ifdef HAVE_BLK_MQ_MAP_QUEUES
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
+#if 0
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
 
 	return blk_mq_rdma_map_queues(set, ctrl->device->dev, 0);
+#else
+	return blk_mq_map_queues(set);
+#endif
 }
+#endif
 
+#ifdef HAVE_BLK_MQ_TAG_SET_HAS_CONST_POS
 static const struct blk_mq_ops nvme_rdma_mq_ops = {
+#else
+static struct blk_mq_ops nvme_rdma_mq_ops = {
+#endif
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
+#ifdef HAVE_BLK_MQ_OPS_MAP_QUEUE
+	.map_queue      = blk_mq_map_queue,
+#endif
 	.init_request	= nvme_rdma_init_request,
 	.exit_request	= nvme_rdma_exit_request,
 	.init_hctx	= nvme_rdma_init_hctx,
+#ifdef HAVE_BLK_MQ_POLL
 	.poll		= nvme_rdma_poll,
+#endif
 	.timeout	= nvme_rdma_timeout,
+#ifdef HAVE_BLK_MQ_MAP_QUEUES
 	.map_queues	= nvme_rdma_map_queues,
+#endif
 };
 
+#ifdef HAVE_BLK_MQ_TAG_SET_HAS_CONST_POS
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
+#else
+static struct blk_mq_ops nvme_rdma_admin_mq_ops = {
+#endif
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
+#ifdef HAVE_BLK_MQ_OPS_MAP_QUEUE
+	.map_queue      = blk_mq_map_queue,
+#endif
+#ifdef HAVE_BLK_MQ_OPS_INIT_REQUEST_HAS_4_PARAMS
 	.init_request	= nvme_rdma_init_request,
+#else
+	.init_request	= nvme_rdma_init_admin_request,
+#endif
+#ifdef HAVE_BLK_MQ_OPS_EXIT_REQUEST_HAS_3_PARAMS
 	.exit_request	= nvme_rdma_exit_request,
+#else
+	.exit_request	= nvme_rdma_exit_admin_request,
+#endif
 	.init_hctx	= nvme_rdma_init_admin_hctx,
 	.timeout	= nvme_rdma_timeout,
 };
@@ -1798,11 +1956,19 @@ static void nvme_rdma_shutdown_ctrl(stru
 	else
 		nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
 
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+#else
+	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
+#endif
 	nvme_rdma_stop_queue(&ctrl->queues[0]);
 	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
 				nvme_cancel_request, &ctrl->ctrl);
+#ifdef HAVE_BLK_MQ_UNQUIESCE_QUEUE
 	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+#else
+	blk_mq_start_stopped_hw_queues(ctrl->ctrl.admin_q, true);
+#endif
 	nvme_rdma_destroy_admin_queue(ctrl, shutdown);
 }
 
