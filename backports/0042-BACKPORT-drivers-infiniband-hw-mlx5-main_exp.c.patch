From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/hw/mlx5/main_exp.c

Change-Id: I21e669ff0bc23df4e68644bfc4b9f71bcced9070
---
 drivers/infiniband/hw/mlx5/main_exp.c | 81 +++++++++++++++++++++++++++++++++++
 1 file changed, 81 insertions(+)

--- a/drivers/infiniband/hw/mlx5/main_exp.c
+++ b/drivers/infiniband/hw/mlx5/main_exp.c
@@ -1188,7 +1188,11 @@ static ssize_t dc_attr_store(struct kobj
 	return dc_attr->store(d, dc_attr, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops dc_sysfs_ops = {
+#else
+static struct sysfs_ops dc_sysfs_ops = {
+#endif
 	.show = dc_attr_show,
 	.store = dc_attr_store
 };
@@ -2215,6 +2219,9 @@ int alloc_and_map_wc(struct mlx5_ib_dev
 	u32 uar_index;
 	struct mlx5_ib_vma_private_data *vma_prv;
 	int err;
+#if defined(CONFIG_X86) && !defined(HAVE_PAT_ENABLED_AS_FUNCTION)
+	pgprot_t prot = __pgprot(0);
+#endif
 
 	if (indx % uars_per_page) {
 		mlx5_ib_warn(dev, "invalid uar index %d, should be system page aligned and there are %d uars per page.\n",
@@ -2223,7 +2230,11 @@ int alloc_and_map_wc(struct mlx5_ib_dev
 	}
 
 #if defined(CONFIG_X86)
+#ifdef HAVE_PAT_ENABLED_AS_FUNCTION
 	if (!pat_enabled()) {
+#else
+	if (pgprot_val(pgprot_writecombine(prot)) == pgprot_val(pgprot_noncached(prot))) {
+#endif
 		mlx5_ib_dbg(dev, "write combine not available\n");
 		return -EPERM;
 	}
@@ -2397,3 +2408,73 @@ int mlx5_ib_exp_set_context_attr(struct
 
 	return 0;
 }
+
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+static int get_command(unsigned long offset)
+{
+	int cmd = (offset >> MLX5_IB_MMAP_CMD_SHIFT) & MLX5_IB_MMAP_CMD_MASK;
+
+	return (cmd == MLX5_IB_EXP_MMAP_CORE_CLOCK) ? MLX5_IB_MMAP_CORE_CLOCK :
+		cmd;
+}
+
+unsigned long mlx5_ib_exp_get_unmapped_area(struct file *file,
+					    unsigned long addr,
+					    unsigned long len,
+					    unsigned long pgoff,
+					    unsigned long flags)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	unsigned long start_addr;
+	unsigned long order;
+	unsigned long command;
+
+	mm = current->mm;
+	if (addr)
+		return current->mm->get_unmapped_area(file, addr, len,
+						      pgoff, flags);
+	command = get_command(pgoff);
+	if (command == MLX5_IB_MMAP_REGULAR_PAGE ||
+	    command == MLX5_IB_MMAP_WC_PAGE ||
+	    command == MLX5_IB_MMAP_NC_PAGE ||
+	    command == MLX5_IB_MMAP_MAP_DC_INFO_PAGE ||
+	    command == MLX5_IB_EXP_ALLOC_N_MMAP_WC ||
+	    command == MLX5_IB_MMAP_CORE_CLOCK ||
+	    command == MLX5_IB_EXP_MMAP_CLOCK_INFO)
+		return current->mm->get_unmapped_area(file, addr, len,
+						      pgoff, flags);
+
+	if (command != MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES &&
+	    command != MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_DEV_NUMA &&
+	    command != MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_CPU_NUMA) {
+		pr_warn("get_unmapped_area unsupported command %ld\n", command);
+		return -EINVAL;
+	}
+
+	order = get_pg_order(pgoff);
+
+	/*
+	 * code is based on the huge-pages get_unmapped_area code
+	 */
+	start_addr = mm->free_area_cache;
+	if (len <= mm->cached_hole_size)
+		start_addr = TASK_UNMAPPED_BASE;
+full_search:
+	addr = ALIGN(start_addr, 1 << order);
+
+	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
+		if (addr > TASK_SIZE - len) {
+			if (start_addr != TASK_UNMAPPED_BASE) {
+				start_addr = TASK_UNMAPPED_BASE;
+				goto full_search;
+			}
+			return -ENOMEM;
+		}
+
+		if (!vma || addr + len <= vma->vm_start)
+			return addr;
+		addr = ALIGN(vma->vm_end, 1 << order);
+	}
+}
+#endif
