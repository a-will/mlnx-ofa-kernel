From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_tx.c

Change-Id: Ifd7132d697b1e3a44af4a61dbc9512fd9f53eae6
---
 drivers/net/ethernet/mellanox/mlx5/core/en_tx.c | 77 +++++++++++++++++++++++--
 1 file changed, 71 insertions(+), 6 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -87,6 +87,7 @@ static void mlx5e_dma_unmap_wqe_err(stru
 	}
 }
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 static inline int mlx5e_get_dscp_up(struct mlx5e_priv *priv, struct sk_buff *skb)
 {
@@ -100,8 +101,9 @@ static inline int mlx5e_get_dscp_up(stru
 	return priv->dcbx_dp.dscp2prio[dscp_cp];
 }
 #endif
+#endif
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 static u16 mlx5e_select_queue_assigned(struct mlx5e_priv *priv,
 				       struct sk_buff *skb)
 {
@@ -148,15 +150,23 @@ fallback:
 }
 #endif
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback)
+#else
+		       void *accel_priv)
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int channel_ix;
 	u16 num_channels;
 	int up;
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	if (priv->channels.params.num_rl_txqs) {
 		u16 ix = mlx5e_select_queue_assigned(priv, skb);
 
@@ -170,14 +180,18 @@ u16 mlx5e_select_queue(struct net_device
 	channel_ix = fallback(dev, skb);
 	up = 0;
 
+#ifdef HAVE_NETDEV_GET_NUM_TC
 	if (!netdev_get_num_tc(dev))
 		return channel_ix;
+#endif
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	if (priv->dcbx_dp.trust_state == MLX5_QPTS_TRUST_DSCP)
 		up = mlx5e_get_dscp_up(priv, skb);
 	else
 #endif
+#endif
 		if (skb_vlan_tag_present(skb))
 			up = skb->vlan_tci >> VLAN_PRIO_SHIFT;
 
@@ -198,14 +212,24 @@ static inline int mlx5e_skb_l2_header_of
 
 static inline int mlx5e_skb_l3_header_offset(struct sk_buff *skb)
 {
+#ifdef FLOW_KEYS_HASH_OFFSET
 	struct flow_keys keys;
+#endif
 
+#ifdef HAVE_SKB_TRANSPORT_HEADER_WAS_SET
 	if (skb_transport_header_was_set(skb))
 		return skb_transport_offset(skb);
-	else if (skb_flow_dissect_flow_keys(skb, &keys, 0))
+#endif
+#ifdef FLOW_KEYS_HASH_OFFSET
+#ifdef HAVE_SKB_FLOW_DISSECT_FLOW_KEYS_HAS_3_PARAMS
+	if (skb_flow_dissect_flow_keys(skb, &keys, 0))
 		return keys.control.thoff;
-	else
-		return mlx5e_skb_l2_header_offset(skb);
+#else
+	if (skb_flow_dissect_flow_keys(skb, &keys))
+		return keys.control.thoff;
+#endif
+#endif
+	return mlx5e_skb_l2_header_offset(skb);
 }
 
 static inline u16 mlx5e_calc_min_inline(enum mlx5_inline_modes mode,
@@ -217,11 +241,13 @@ static inline u16 mlx5e_calc_min_inline(
 	switch (mode) {
 	case MLX5_INLINE_MODE_NONE:
 		return 0;
+#ifdef HAVE_ETH_GET_HEADLEN
 	case MLX5_INLINE_MODE_TCP_UDP:
 		hlen = eth_get_headlen(skb->data, skb_headlen(skb));
 		if (hlen == ETH_HLEN && !vlan_present)
 			hlen += VLAN_HLEN;
 		break;
+#endif
 	case MLX5_INLINE_MODE_IP:
 		/* When transport header is set to zero, it means no transport
 		 * header. When transport header is set to 0xff's, it means
@@ -260,7 +286,11 @@ static inline void mlx5e_insert_vlan(voi
 
 	memcpy(vhdr, *skb_data, cpy1_sz);
 	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy1_sz);
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	vhdr->h_vlan_proto = skb->vlan_proto;
+#else
+	vhdr->h_vlan_proto = cpu_to_be16(ETH_P_8021Q);
+#endif
 	vhdr->h_vlan_TCI = cpu_to_be16(skb_vlan_tag_get(skb));
 	memcpy(&vhdr->h_vlan_encapsulated_proto, *skb_data, cpy2_sz);
 	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy2_sz);
@@ -271,6 +301,7 @@ mlx5e_txwqe_build_eseg_csum(struct mlx5e
 {
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
 		eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 		if (skb->encapsulation) {
 			eseg->cs_flags |= MLX5_ETH_WQE_L3_INNER_CSUM |
 					  MLX5_ETH_WQE_L4_INNER_CSUM;
@@ -279,6 +310,9 @@ mlx5e_txwqe_build_eseg_csum(struct mlx5e
 			eseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;
 			sq->stats.csum_partial++;
 		}
+#else
+		eseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;
+#endif
 	} else
 		sq->stats.csum_none++;
 }
@@ -291,15 +325,23 @@ mlx5e_txwqe_build_eseg_gso(struct mlx5e_
 
 	eseg->mss    = cpu_to_be16(skb_shinfo(skb)->gso_size);
 
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER) && defined(HAVE_SK_BUFF_ENCAPSULATION)
 	if (skb->encapsulation) {
+#ifdef HAVE_SKB_INNER_TRANSPORT_OFFSET
 		ihs = skb_inner_transport_offset(skb) + inner_tcp_hdrlen(skb);
+#else
+		ihs = skb_inner_transport_header(skb) - skb->data + inner_tcp_hdrlen(skb);
+#endif
 		sq->stats.tso_inner_packets++;
 		sq->stats.tso_inner_bytes += skb->len - ihs;
 	} else {
+#endif
 		ihs = skb_transport_offset(skb) + tcp_hdrlen(skb);
 		sq->stats.tso_packets++;
 		sq->stats.tso_bytes += skb->len - ihs;
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER) && defined(HAVE_SK_BUFF_ENCAPSULATION)
 	}
+#endif
 
 	*num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
 	return ihs;
@@ -368,8 +410,16 @@ mlx5e_txwqe_complete(struct mlx5e_txqsq
 
 	netdev_tx_sent_queue(sq->txq, num_bytes);
 
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+#else
+	if (unlikely(skb_shinfo(skb)->tx_flags.flags & SKBTX_HW_TSTAMP))
+#endif
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+#else
+		skb_shinfo(skb)->tx_flags.flags |= SKBTX_IN_PROGRESS;
+#endif
 
 	sq->pc += wi->num_wqebbs;
 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, MLX5E_SQ_STOP_ROOM))) {
@@ -377,7 +427,9 @@ mlx5e_txwqe_complete(struct mlx5e_txqsq
 		sq->stats.stopped++;
 	}
 
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+#endif
 		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
 
 	/* fill sq edge with nops to avoid wqe wrap around */
@@ -419,7 +471,9 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		sq->stats.packets++;
 	}
 	sq->stats.bytes += num_bytes;
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	sq->stats.xmit_more += skb->xmit_more;
+#endif
 
 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
 	if (ihs) {
@@ -435,8 +489,10 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		ds_cnt += DIV_ROUND_UP(ihs - sizeof(eseg->inline_hdr.start), MLX5_SEND_WQE_DS);
 	} else if (vlan_present) {
 		eseg->insert.type = cpu_to_be16(MLX5_ETH_WQE_INSERT_VLAN);
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		if (skb->vlan_proto == cpu_to_be16(ETH_P_8021AD))
 			eseg->insert.type |= cpu_to_be16(MLX5_ETH_WQE_SVLAN);
+#endif
 		eseg->insert.vlan_tci = cpu_to_be16(skb_vlan_tag_get(skb));
 		sq->stats.added_vlan_packets++;
 	}
@@ -561,7 +617,11 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *c
 				continue;
 			}
 
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 			if (unlikely(skb_shinfo(skb)->tx_flags &
+#else
+			if (unlikely(skb_shinfo(skb)->tx_flags.flags &
+#endif
 				     SKBTX_HW_TSTAMP)) {
 				struct skb_shared_hwtstamps hwts = {};
 
@@ -581,7 +641,11 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *c
 			npkts++;
 			nbytes += wi->num_bytes;
 			sqcc += wi->num_wqebbs;
+#ifdef HAVE_NAPI_CONSUME_SKB
 			napi_consume_skb(skb, napi_budget);
+#else
+			dev_kfree_skb(skb);
+#endif
 		} while (!last_wqe);
 
 	} while ((++i < MLX5E_TX_CQ_POLL_BUDGET) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));
@@ -698,8 +762,9 @@ netdev_tx_t mlx5i_sq_xmit(struct mlx5e_t
 	}
 
 	sq->stats.bytes += num_bytes;
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	sq->stats.xmit_more += skb->xmit_more;
-
+#endif
 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
 	if (ihs) {
 		memcpy(eseg->inline_hdr.start, skb_data, ihs);
