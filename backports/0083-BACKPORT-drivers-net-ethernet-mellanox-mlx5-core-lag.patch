From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/lag.c

Change-Id: I6976b7c4f8a03d5b1e2aa74999ff02033e75e55b
---
 drivers/net/ethernet/mellanox/mlx5/core/lag.c | 165 ++++++++++++++++++++++++++
 1 file changed, 165 insertions(+)

--- a/drivers/net/ethernet/mellanox/mlx5/core/lag.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lag.c
@@ -36,6 +36,22 @@
 #include "mlx5_core.h"
 #include "eswitch.h"
 
+#ifdef MLX_USE_LAG_COMPAT
+#define MLX_IMPL_LAG_EVENTS
+#include <net/bonding.h>
+
+#include <linux/device.h>
+#include <net/rtnetlink.h>
+#include <net/sock.h>
+#include "en.h"
+#endif
+
+#if defined(MLX_USE_LAG_COMPAT) || defined(HAVE_LAG_TX_TYPE)
+#define MLX_LAG_SUPPORTED
+#endif
+
+#ifdef MLX_LAG_SUPPORTED
+
 enum {
 	MLX5_LAG_FLAG_BONDED = 1 << 0,
 };
@@ -69,6 +85,94 @@ struct mlx5_lag {
  * under it).
  */
 static DEFINE_MUTEX(lag_mutex);
+#endif
+
+#ifdef MLX_USE_LAG_COMPAT
+#undef  register_netdevice_notifier
+#undef  unregister_netdevice_notifier
+#define register_netdevice_notifier  		mlx5_lag_compat_register_netdev_notifier
+#define unregister_netdevice_notifier		mlx5_lag_compat_unregister_netdev_notifier
+#undef register_netdevice_notifier_rh
+#undef unregister_netdevice_notifier_rh
+#define register_netdevice_notifier_rh          mlx5_lag_compat_register_netdev_notifier
+#define unregister_netdevice_notifier_rh        mlx5_lag_compat_unregister_netdev_notifier
+
+#undef  netdev_notifier_info_to_dev
+#define netdev_notifier_info_to_dev		netdev_notifier_info_to_dev_v2
+
+#define MLX5_LAG_COMPAT_MAX_LAGDEVS		0x8
+
+static int mlx5_lag_netdev_event(struct notifier_block *this,
+				 unsigned long event, void *ptr);
+
+static struct mlx5_lag *mlx5_lag_compat_ldevs[MLX5_LAG_COMPAT_MAX_LAGDEVS] = {};
+static int mlx5_lag_compat_reg_ldevs = 0;
+
+static void mlx5_lag_compat_netdev_event(unsigned long event, void *ptr)
+{
+	struct mlx5_lag *ldev;
+	int i;
+
+	for (i = 0; i < MLX5_LAG_COMPAT_MAX_LAGDEVS; ++i) {
+		ldev = mlx5_lag_compat_ldevs[i];
+		if (!ldev)
+			continue;
+		mlx5_lag_netdev_event(&ldev->nb, event, ptr);
+	}
+}
+
+static int mlx5_lag_compat_register_netdev_notifier(struct notifier_block *nb)
+{
+	struct mlx5_lag *ldev = container_of(nb, struct mlx5_lag, nb);
+	int err = 0, i;
+
+	if (!mlx5_lag_compat_reg_ldevs)
+		mlx_lag_compat_events_open(mlx5_lag_compat_netdev_event);
+
+	rtnl_lock();
+	for (i = 0; i < MLX5_LAG_COMPAT_MAX_LAGDEVS; ++i) {
+		if (mlx5_lag_compat_ldevs[i])
+			continue;
+
+		mlx5_lag_compat_ldevs[i] = ldev;
+		break;
+	}
+
+	if (i == MLX5_LAG_COMPAT_MAX_LAGDEVS) {
+		err = -EINVAL;
+		goto unlock;
+	}
+
+	++mlx5_lag_compat_reg_ldevs;
+
+unlock:
+	rtnl_unlock();
+	return err;
+}
+
+static void mlx5_lag_compat_unregister_netdev_notifier(struct notifier_block *nb)
+{
+	struct mlx5_lag *ldev = container_of(nb, struct mlx5_lag, nb);
+	int i;
+
+	rtnl_lock();
+	for (i = 0; i < MLX5_LAG_COMPAT_MAX_LAGDEVS; ++i) {
+		if (mlx5_lag_compat_ldevs[i] != ldev)
+			continue;
+
+		mlx5_lag_compat_ldevs[i] = NULL;
+		break;
+	}
+
+	--mlx5_lag_compat_reg_ldevs;
+	rtnl_unlock();
+
+	if (!mlx5_lag_compat_reg_ldevs)
+		mlx_lag_compat_events_close();
+}
+#endif
+
+#ifdef MLX_LAG_SUPPORTED
 
 static int mlx5_cmd_create_lag(struct mlx5_core_dev *dev, u8 remap_port1,
 			       u8 remap_port2)
@@ -110,26 +214,35 @@ static int mlx5_cmd_destroy_lag(struct m
 
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 }
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 
 int mlx5_cmd_create_vport_lag(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return -EOPNOTSUPP;
+#else
 	u32  in[MLX5_ST_SZ_DW(create_vport_lag_in)]  = {0};
 	u32 out[MLX5_ST_SZ_DW(create_vport_lag_out)] = {0};
 
 	MLX5_SET(create_vport_lag_in, in, opcode, MLX5_CMD_OP_CREATE_VPORT_LAG);
 
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_cmd_create_vport_lag);
 
 int mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return -EOPNOTSUPP;
+#else
 	u32  in[MLX5_ST_SZ_DW(destroy_vport_lag_in)]  = {0};
 	u32 out[MLX5_ST_SZ_DW(destroy_vport_lag_out)] = {0};
 
 	MLX5_SET(destroy_vport_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_VPORT_LAG);
 
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_cmd_destroy_vport_lag);
 
@@ -144,6 +257,7 @@ static int mlx5_cmd_query_cong_counter(s
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, out_size);
 }
 
+#ifdef MLX_LAG_SUPPORTED
 static struct mlx5_lag *mlx5_lag_dev_get(struct mlx5_core_dev *dev)
 {
 	return dev->priv.lag;
@@ -361,15 +475,21 @@ static int mlx5_handle_changeupper_event
 	 * as slaves (e.g., if a new slave is added to a master that bonds two
 	 * of our netdevs, we should unbond).
 	 */
+#ifdef for_each_netdev_in_bond_rcu
 	rcu_read_lock();
 	for_each_netdev_in_bond_rcu(upper, ndev_tmp) {
+#else
+	for_each_netdev_in_bond(upper, ndev_tmp) {
+#endif
 		idx = mlx5_lag_dev_get_netdev_idx(ldev, ndev_tmp);
 		if (idx > -1)
 			bond_status |= (1 << idx);
 
 		num_slaves++;
 	}
+#ifdef for_each_netdev_in_bond_rcu
 	rcu_read_unlock();
+#endif
 
 	/* None of this lagdev's netdevs are slaves of this master. */
 	if (!(bond_status & 0x3))
@@ -554,10 +674,12 @@ static ssize_t mlx5_lag_set_enabled(stru
 
 static DEVICE_ATTR(roce_lag_enable, 0644, mlx5_lag_show_enabled, mlx5_lag_set_enabled);
 static struct device_attribute *mlx5_lag_dev_attrs = &dev_attr_roce_lag_enable;
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 
 /* Must be called with intf_mutex held */
 void mlx5_lag_add(struct mlx5_core_dev *dev, struct net_device *netdev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev = NULL;
 	struct mlx5_core_dev *tmp_dev;
 
@@ -597,11 +719,13 @@ void mlx5_lag_add(struct mlx5_core_dev *
 
 remove_file:
 	device_remove_file(&dev->pdev->dev, mlx5_lag_dev_attrs);
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 /* Must be called with intf_mutex held */
 void mlx5_lag_remove(struct mlx5_core_dev *dev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
 	int i;
 
@@ -626,10 +750,14 @@ void mlx5_lag_remove(struct mlx5_core_de
 		cancel_delayed_work_sync(&ldev->bond_work);
 		mlx5_lag_dev_free(ldev);
 	}
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 bool mlx5_lag_is_active(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return false;
+#else
 	struct mlx5_lag *ldev;
 	bool res;
 
@@ -639,11 +767,13 @@ bool mlx5_lag_is_active(struct mlx5_core
 	mutex_unlock(&lag_mutex);
 
 	return res;
+#endif
 }
 EXPORT_SYMBOL(mlx5_lag_is_active);
 
 void mlx5_lag_update(struct mlx5_core_dev *dev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
 
 	mlx5_dev_list_lock();
@@ -655,11 +785,15 @@ void mlx5_lag_update(struct mlx5_core_de
 
 unlock:
 	mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 
 struct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return NULL;
+#else
 	struct net_device *ndev = NULL;
 	struct mlx5_lag *ldev;
 
@@ -682,11 +816,15 @@ unlock:
 	mutex_unlock(&lag_mutex);
 
 	return ndev;
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_lag_get_roce_netdev);
 
 bool mlx5_lag_intf_add(struct mlx5_interface *intf, struct mlx5_priv *priv)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return false;
+#else
 	struct mlx5_core_dev *dev = container_of(priv, struct mlx5_core_dev,
 						 priv);
 	struct mlx5_lag *ldev;
@@ -700,6 +838,7 @@ bool mlx5_lag_intf_add(struct mlx5_inter
 
 	/* If bonded, we do not add an IB device for PF1. */
 	return false;
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 
 int mlx5_lag_query_cong_counters(struct mlx5_core_dev *dev,
@@ -709,7 +848,9 @@ int mlx5_lag_query_cong_counters(struct
 {
 	int outlen = MLX5_ST_SZ_BYTES(query_cong_statistics_out);
 	struct mlx5_core_dev *mdev[MLX5_MAX_PORTS];
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
+#endif
 	int num_ports;
 	int ret, i, j;
 	void *out;
@@ -720,6 +861,7 @@ int mlx5_lag_query_cong_counters(struct
 
 	memset(values, 0, sizeof(*values) * num_counters);
 
+#ifdef MLX_LAG_SUPPORTED
 	mutex_lock(&lag_mutex);
 	ldev = mlx5_lag_dev_get(dev);
 	if (ldev && mlx5_lag_is_bonded(ldev)) {
@@ -730,6 +872,10 @@ int mlx5_lag_query_cong_counters(struct
 		num_ports = 1;
 		mdev[0] = dev;
 	}
+#else
+	num_ports = 1;
+	mdev[0] = dev;
+#endif
 
 	for (i = 0; i < num_ports; ++i) {
 		ret = mlx5_cmd_query_cong_counter(mdev[i], false, out, outlen);
@@ -741,7 +887,9 @@ int mlx5_lag_query_cong_counters(struct
 	}
 
 unlock:
+#ifdef MLX_LAG_SUPPORTED
 	mutex_unlock(&lag_mutex);
+#endif
 	kvfree(out);
 	return ret;
 }
@@ -759,11 +907,14 @@ int mlx5_lag_modify_cong_params(struct m
 				void *in, int in_size)
 {
 	struct mlx5_core_dev *mdev[MLX5_MAX_PORTS];
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
+#endif
 	int num_ports;
 	int ret;
 	int i;
 
+#ifdef MLX_LAG_SUPPORTED
 	mutex_lock(&lag_mutex);
 	ldev = mlx5_lag_dev_get(dev);
 	if (ldev && mlx5_lag_is_bonded(ldev)) {
@@ -774,6 +925,10 @@ int mlx5_lag_modify_cong_params(struct m
 		num_ports = 1;
 		mdev[0] = dev;
 	}
+#else
+	num_ports = 1;
+	mdev[0] = dev;
+#endif
 
 	for (i = 0; i < num_ports; i++) {
 		ret = mlx5_cmd_modify_cong_params(mdev[i], in, in_size);
@@ -782,13 +937,18 @@ int mlx5_lag_modify_cong_params(struct m
 	}
 
 unlock:
+#ifdef MLX_LAG_SUPPORTED
 	mutex_unlock(&lag_mutex);
+#endif
 	return ret;
 }
 EXPORT_SYMBOL(mlx5_lag_modify_cong_params);
 
 struct mlx5_core_dev *mlx5_lag_get_peer_mdev(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return NULL;
+#else
 	struct mlx5_core_dev *peer_dev = NULL;
 	struct mlx5_lag *ldev;
 
@@ -802,10 +962,14 @@ struct mlx5_core_dev *mlx5_lag_get_peer_
 unlock:
 	mutex_unlock(&lag_mutex);
 	return peer_dev;
+#endif
 }
 
 struct net_device *mlx5_lag_get_peer_netdev(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return NULL;
+#else
 	struct net_device *peer_ndev = NULL;
 	struct mlx5_lag *ldev;
 
@@ -819,4 +983,5 @@ struct net_device *mlx5_lag_get_peer_net
 unlock:
 	mutex_unlock(&lag_mutex);
 	return peer_ndev;
+#endif
 }
